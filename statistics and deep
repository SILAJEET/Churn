import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import csv

data=pd.read_csv(r"bank-additional.csv",sep=";")

from sklearn.preprocessing import LabelEncoder

le1=LabelEncoder()
data.job=le1.fit_transform(data.job)

le2=LabelEncoder()
data.marital=le2.fit_transform(data.marital)

le3=LabelEncoder()
data.education=le3.fit_transform(data.education)

le4=LabelEncoder()
data.default=le4.fit_transform(data.default)

le5=LabelEncoder()
data.housing=le5.fit_transform(data.housing)

le6=LabelEncoder()
data.loan=le6.fit_transform(data.loan)

le7=LabelEncoder()
data.contact=le6.fit_transform(data.contact)

le8=LabelEncoder()
data.month=le8.fit_transform(data.month)

le9=LabelEncoder()
data.day_of_week=le9.fit_transform(data.day_of_week)

le10=LabelEncoder()
data.poutcome=le10.fit_transform(data.poutcome)

le11=LabelEncoder()
data.y=le11.fit_transform(data.y)

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix

from sklearn import metrics

ip=data.drop(['y'],axis=1)
op=data['y']
data.age[data.y==1].count()
data.age[data.y==1].mean()
data.age[data.y==1].median()
data.age[data.y==1].mode()
data.age[data.y==1].var()
data.age[data.y==1].std()
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score

from sklearn.model_selection import train_test_split
xtr,xts,ytr,yts=train_test_split(ip,op,test_size=0.2)

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
sc.fit(xtr)
xtr=sc.transform(xtr)
xts=sc.transform(xts)

from imblearn.over_sampling import SMOTE
sm=SMOTE(random_state=2)


from sklearn.ensemble import AdaBoostClassifier
abc=AdaBoostClassifier(n_estimators=50,learning_rate=1)
model=abc.fit(xtr,ytr)
accuracy=abc.score(xts,yts)
print('accuracy is ',accuracy)
y_pred=model.predict(xts)
print('F1 score:', f1_score(yts, y_pred,
                            average='weighted'))

print('Recall:', recall_score(yts, y_pred,
                              average='weighted'))

print('Precision:', precision_score(yts,y_pred,
                                    average='weighted'))

predictions = pd.DataFrame(y_pred)
predictions[0].value_counts()
from sklearn.metrics import classification_report
classification_report(yts,y_pred)
cm=metrics.confusion_matrix(yts,y_pred)
print(cm)

from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
import sklearn.metrics as metrics
probs = abc.predict_proba(xts)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(yts, preds)
roc_auc = metrics.auc(fpr, tpr)
print( "The AUC FOR ADABOOST IS",roc_auc)
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' %roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
alg1=LogisticRegression()
alg1.fit(xtr, ytr)
accuracy=alg1.score(xts,yts)
print('accuracy is ',accuracy)



yp=alg1.predict(xts)

print('F1 score:', f1_score(yts, yp,average='weighted'))
                                       

print('Recall:', recall_score(yts, yp,
                              average='weighted'))

print('Precision:', precision_score(yts,yp,
                                    average='weighted'))


predictions = pd.DataFrame(yp)
predictions[0].value_counts()
from sklearn.metrics import classification_report
classification_report(yts,yp)
cm=metrics.confusion_matrix(yts,yp)
print(cm)

from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
import sklearn.metrics as metrics
probs = alg1.predict_proba(xts)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(yts, preds)
roc_auc = metrics.auc(fpr, tpr)
print( "The AUC FOR LOGISTIC REGRESSION IS",roc_auc)
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' %roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()


from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier(max_depth=5,n_estimators=5)
rf.fit(xtr,ytr)
accuracy=rf.score(xts,yts)
print('accuracy is ',accuracy)
yp=rf.predict(xts)


predictions = pd.DataFrame(yp)
predictions[0].value_counts()
from sklearn import metrics
from sklearn.metrics import classification_report
classification_report(yts,yp)
cm=metrics.confusion_matrix(yts,yp)
print(cm)

print('F1 score:', f1_score(yts, yp,
                            average='weighted'))

print('Recall:', recall_score(yts, yp,
                              average='weighted'))

print('Precision:', precision_score(yts,yp,
                                    average='weighted'))
probs = rf.predict_proba(xts)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(yts, preds)
roc_auc = metrics.auc(fpr, tpr)
print( "The AUC for RANDOM FOREST CLASSIFER is",roc_auc)
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' %roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""from sklearn.svm import SVC
classifier = SVC(kernel = 'rbf', random_state = 0)
classifier.fit(xtr, ytr)
accuracy=rf.score(xts,yts)
print('accuracy is ',accuracy)
yp=classifier.predict(xts)
predictions = pd.DataFrame(yp)
predictions[0].value_counts()
from sklearn import metrics
from sklearn.metrics import classification_report
classification_report(yts,yp)
cm=metrics.confusion_matrix(yts,yp)
print(cm)


probs = classifier.predict_proba(X_test)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, preds)
roc_auc = metrics.auc(fpr, tpr)
print("The AUC FOR SVM  IS",roc_auc)
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' %roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()"""

from sklearn.tree import DecisionTreeClassifier
classifier1 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier1.fit(xtr, ytr)
accuracy=classifier1.score(xts,yts)
print('accuracy is ',accuracy)
yp=classifier1.predict(xts)

print('F1 score:', f1_score(yts, yp,
                            average='weighted'))

print('Recall:', recall_score(yts, yp,
                              average='weighted'))

print('Precision:', precision_score(yts,yp,
                                    average='weighted'))
predictions = pd.DataFrame(yp)
predictions[0].value_counts()
from sklearn import metrics
from sklearn.metrics import classification_report
classification_report(yts,yp)
cm=metrics.confusion_matrix(yts,yp)
print(cm)


probs = classifier1.predict_proba(xts)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(yts, preds)
roc_auc = metrics.auc(fpr, tpr)
print( "The AUC for DECISION TREE is",roc_auc)
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' %roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

